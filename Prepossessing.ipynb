{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908d1bd3",
   "metadata": {},
   "source": [
    "# In the notebook _Learning Twitter API_, we download 100 tweets from 100 people (Approx because some private users). Here we are going to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c1d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d92b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all paths\n",
    "folderpath = \"./tweets\"\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7876cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "# Looping through the files and extracting their tweets\n",
    "for path in filepaths:\n",
    "    username = path.split('/')[2][:-4]\n",
    "    try:\n",
    "        # Grabbing just the tweets\n",
    "        tweets[username] = list(pd.read_csv(path, sep=',', engine='python', names=[0,1,2])[2])\n",
    "    #print(username)\n",
    "    except:\n",
    "        print(f'This user is empty {username}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca16d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "# Tokenizing with nltk\n",
    "tt = TweetTokenizer()\n",
    "for username in tweets:\n",
    "    tokens[username] = []\n",
    "    for tweet in tweets[username]:\n",
    "        tokenized_tweet = tt.tokenize(tweet)\n",
    "        tokens[username].append(tokenized_tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c854dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the useless words in spanish\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "sw = nltk.corpus.stopwords.words(\"spanish\")\n",
    "sw += nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e73efc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–¿¡”“'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to clean punctuation sings too, i'm adding spanish ones\n",
    "punc = string.punctuation + '–¿¡”“'\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3920d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a function that tells me if a string is a number\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e2677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning this useless words\n",
    "filtered_tweets = {}\n",
    "for username in tweets:\n",
    "    filtered_tweets[username] = []\n",
    "    for index, tweet in enumerate(tokens[username]):\n",
    "        filtered_tweet = [word for word in tokens[username][index] if word not in sw and not is_number(word)]\n",
    "        no_punc = [i for i in  filtered_tweet if i not in punc]\n",
    "        filtered_tweets[username].append(list(filter(None, no_punc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "998b2889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CynthiaRLandin 1\n",
      "RT @Proteccion_leon: Se da atención por reporte de inundación en la colonia Brisas del Campestre en la zona poniente de nuestro municipio,…\n",
      "\n",
      "['RT', '@Proteccion_leon', ':', 'Se', 'da', 'atención', 'por', 'reporte', 'de', 'inundación', 'en', 'la', 'colonia', 'Brisas', 'del', 'Campestre', 'en', 'la', 'zona', 'poniente', 'de', 'nuestro', 'municipio', ',', '…']\n",
      "\n",
      "['RT', '@Proteccion_leon', 'Se', 'da', 'atención', 'reporte', 'inundación', 'colonia', 'Brisas', 'Campestre', 'zona', 'poniente', 'municipio', '…']\n"
     ]
    }
   ],
   "source": [
    "# Just to compare all the tweets and the process with a random user\n",
    "random_username = random.choice(list(tweets.keys()))\n",
    "random_number = random.randint(0,4)\n",
    "print(random_username, random_number)\n",
    "\n",
    "print(tweets[random_username][random_number])\n",
    "print()\n",
    "print(tokens[random_username][random_number])\n",
    "print()\n",
    "print(filtered_tweets[random_username][random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b40f1f",
   "metadata": {},
   "source": [
    "## La última lista que se ve en la celda de arriba, es la lista final que usaremos para obtener de qué se trata el tweet, notese que están oganizados por usuario y por número de tweet c:, la lista se llama \"filtered_tweets\", y se supone que ese es todo el preprocesamiento que se necesita para aplicar lo que queremos C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99426918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
